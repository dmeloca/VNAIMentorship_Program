{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Porque es util usar un dataloader de pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pensemos en un dataset de 100_000 GB. Podríamos cargar esto directamente en memoría ocupando esa cantidad de memoría o podríamos solo cargar lo que vamos a utilizar. Es decir podríamos cargarlo por partes y cuando se solicite.\n",
    "\n",
    "Aquí esto se relaciona con los iteradores y generadores de python.\n",
    "Esto es un objeto de python que nos ayuda a crear secuencias \"_uno a uno_\", especialmente cuando manejamos grandes volumenes de datos o secuencias infitas, por ejemplo:\n",
    "\n",
    "- Procesar grandes cantidad de datos, como el de entrenar una red para vision por computadora.\n",
    "- Podemos crear una secuencia infinta como los numeros de pi o fibonaci.\n",
    "- Procesar los datos solo bajo demanda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementemos nuestro propio iterador\n",
    "\n",
    "- Principalmente un iterable es un objeto que usa los metodos especiales `__iter__()` y `__next__()`, en especifico, nuestro iteador es el que usa el metodo `__next__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Estoy en init\n",
      "\t Estoy en iterable 0\n",
      "\t Estoy en el iterador 0\n",
      "\t incrementamos el indice en 1\n",
      "1: a\n",
      "\t Estoy en el iterador 1\n",
      "\t incrementamos el indice en 2\n",
      "2: b\n",
      "\t Estoy en el iterador 2\n",
      "\t incrementamos el indice en 3\n",
      "3: c\n",
      "\t Estoy en el iterador 3\n",
      "\t Ya nos salimos del indice del objeto\n",
      "\n",
      "Enumerate con el objeto integrado de python:\n",
      "\n",
      "0: a\n",
      "1: b\n",
      "2: c\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Queremos contruir una funcion que me permita moverme por mi\n",
    "objeto indexado y que me muestre el indice de este elemento:\n",
    "\n",
    "    for index, letter in enumerate('abc'):\n",
    "        print(f'{index}: {letter}')\n",
    "\n",
    "Lo que se quiere aca es crear nuestra propia clase enumerate.\n",
    "'''\n",
    "\n",
    "class MyEnumerate():\n",
    "    def __init__(self, data):\n",
    "        print('\\t Estoy en init')\n",
    "        self.data = data\n",
    "        self.index = 0 \n",
    "    def __iter__(self):\n",
    "        print(f'\\t Estoy en iterable {self.index}')\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        print(f'\\t Estoy en el iterador {self.index}') \n",
    "        if self.index >= len(self.data):\n",
    "            print('\\t Ya nos salimos del indice del objeto')\n",
    "            raise StopIteration\n",
    "\n",
    "        value = self.data[self.index]\n",
    "        self.index += 1\n",
    "        print(f'\\t incrementamos el indice en {self.index}')\n",
    "\n",
    "        return self.index, value\n",
    "\n",
    "for index, letter in MyEnumerate('abc'):\n",
    "    print(f'{index}: {letter}')\n",
    "\n",
    "print(\"\\nEnumerate con el objeto integrado de python:\\n\")\n",
    "for i, l in enumerate('abc'):\n",
    "    print(f'{i}: {l}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora creemos este iterador con la forma de un generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tindex:0 value:0\n",
      "\tindex:1 value:1\n",
      "\tindex:2 value:2\n",
      "\tindex:3 value:3\n",
      "\tindex:4 value:4\n",
      "\tindex:5 value:5\n",
      "\tindex:6 value:6\n",
      "\tindex:7 value:7\n",
      "\tindex:8 value:8\n",
      "\tindex:9 value:9\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Redefine MyEnumerate as a generator function, rather than as a class\n",
    "'''\n",
    "\n",
    "def MyEnumerate_gen(secuence):\n",
    "    idx = 0\n",
    "    for char  in secuence:\n",
    "        yield (idx, char)\n",
    "        idx +=1\n",
    "\n",
    "for index, value in MyEnumerate_gen([i for i in range(10)]):\n",
    "    print(f\"\\tindex:{index} value:{value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veamos otro uso de los generadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "13\n",
      "21\n",
      "34\n",
      "55\n",
      "89\n",
      "144\n",
      "233\n",
      "377\n",
      "610\n",
      "987\n",
      "1597\n",
      "2584\n",
      "4181\n",
      "6765\n",
      "10946\n",
      "17711\n",
      "28657\n",
      "46368\n",
      "75025\n",
      "121393\n",
      "196418\n",
      "317811\n",
      "514229\n",
      "832040\n",
      "1346269\n",
      "2178309\n",
      "3524578\n",
      "5702887\n",
      "9227465\n",
      "14930352\n",
      "24157817\n",
      "39088169\n",
      "63245986\n",
      "102334155\n",
      "165580141\n",
      "267914296\n",
      "433494437\n",
      "701408733\n",
      "1134903170\n",
      "1836311903\n",
      "2971215073\n",
      "4807526976\n",
      "7778742049\n",
      "12586269025\n"
     ]
    }
   ],
   "source": [
    "from typing import Generator\n",
    "def fib6(n: int) -> Generator[int, None, None]:\n",
    "    yield 0 # special case\n",
    "    if n > 0: yield 1 # special case\n",
    "    last: int = 0 # initially set to fib(0)\n",
    "    next: int = 1 # initially set to fib(1)\n",
    "    for _ in range(1, n):\n",
    "        last, next = next, last + next # fib(n) = fib(n-1) + fib(n-2)\n",
    "        yield next # main generation step\n",
    "if __name__ == \"__main__\":\n",
    "    for i in fib6(50):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Característica         | Iterable                      | Generador                         |\n",
    "|------------------------|-------------------------------|-----------------------------------|\n",
    "| **Almacenamiento**      | Almacena todos los datos       | No almacena, produce bajo demanda |\n",
    "| **Eficiencia de memoria**| Consume más memoria si los datos son grandes | Muy eficiente, genera datos uno por uno |\n",
    "| **Acceso a datos**      | Puedes acceder varias veces    | Solo puedes acceder una vez       |\n",
    "| **Implementación**      | Cualquier objeto con `__iter__` | Usa `yield` dentro de una función |\n",
    "| **Ejemplo**             | Listas, cadenas, tuplas       | Funciones generadoras             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del dataloader en python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76, 42, 21, 20, 29, 80, 37, 82, 69, 89]\n",
      "[34, 56, 14, 5, 49, 30, 67, 12, 17, 74]\n",
      "[15, 31, 28, 45, 9, 24, 84, 66, 61, 54]\n",
      "[59, 81, 71, 65, 85, 70, 92, 83, 78, 64]\n",
      "[3, 99, 87, 44, 73, 77, 96, 11, 1, 72]\n",
      "[68, 91, 25, 19, 26, 10, 46, 50, 53, 52]\n",
      "[97, 7, 16, 2, 60, 93, 57, 38, 63, 36]\n",
      "[22, 41, 8, 55, 79, 48, 51, 23, 40, 58]\n",
      "[86, 98, 18, 33, 95, 94, 75, 43, 88, 35]\n",
      "[13, 47, 62, 6, 90, 0, 27, 39, 32, 4]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_samples = len(self.dataset)\n",
    "        self.n_batches = math.ceil(self.n_samples / self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            # Si se desea barajar el dataset\n",
    "            import random\n",
    "            random.shuffle(self.dataset)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= self.n_samples:\n",
    "            raise StopIteration\n",
    "\n",
    "        # Obtener el lote actual (batch)\n",
    "        batch = self.dataset[self.current_index:self.current_index + self.batch_size]\n",
    "        self.current_index += self.batch_size\n",
    "        return batch\n",
    "\n",
    "# Simulamos un dataset con 100 elementos (puede ser cualquier cosa, como imágenes)\n",
    "dataset = list(range(100))\n",
    "\n",
    "# Creamos el DataLoader con un tamaño de lote de 10\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Iteramos sobre los lotes de datos\n",
    "for batch in dataloader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Como estan implementados los dataloaders de pytorch como para que no se usen estos de python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los **DataLoaders** en PyTorch están diseñados para manejar eficientemente grandes volúmenes de datos durante el entrenamiento de modelos de machine learning. Si bien conceptualmente son similares a los iteradores y generadores en Python, los **DataLoaders de PyTorch** tienen características adicionales que los hacen más adecuados para tareas de aprendizaje profundo. Vamos a explorar cómo están implementados y por qué son diferentes de los iteradores básicos de Python.\n",
    "\n",
    "### 1. **Paralelismo y Multiprocesamiento**\n",
    "Uno de los aspectos clave de los **DataLoaders de PyTorch** es su capacidad para cargar los datos en paralelo utilizando múltiples procesos. Esto es crítico para entrenar modelos de machine learning en GPUs, donde la velocidad de cómputo es mucho mayor que la velocidad a la que se pueden cargar los datos desde el disco. El DataLoader de PyTorch puede aprovechar el paralelismo con el parámetro `num_workers`, que permite especificar cuántos procesos se utilizarán para cargar los datos en paralelo.\n",
    "\n",
    "- En **Python puro**, al usar generadores o iteradores, los datos se cargan de manera secuencial y en un solo hilo de ejecución. Esto no aprovecha al máximo los recursos de hardware disponibles.\n",
    "  \n",
    "#### Ejemplo:\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset sería un conjunto de datos definido por el usuario o uno predefinido por PyTorch\n",
    "data_loader = DataLoader(dataset, batch_size=32, num_workers=4)  # Usa 4 procesos en paralelo\n",
    "```\n",
    "\n",
    "- El parámetro `num_workers` le dice al **DataLoader** cuántos procesos paralelos usar para cargar los datos. Mientras tanto, el modelo puede entrenarse sin tener que esperar a que se carguen los datos del siguiente lote.\n",
    "\n",
    "### 2. **Soporte para batches**\n",
    "En PyTorch, el **DataLoader** está diseñado para agrupar automáticamente los datos en **batches** (lotes), lo cual es crucial para el entrenamiento de modelos. El entrenamiento de un modelo generalmente implica procesar lotes de datos a la vez (por ejemplo, 32 o 64 muestras), en lugar de una sola muestra por iteración.\n",
    "\n",
    "- Si usas generadores o iteradores en Python sin un **DataLoader**, necesitarías implementar manualmente la lógica de agrupación en lotes.\n",
    "\n",
    "#### Ejemplo:\n",
    "```python\n",
    "data_loader = DataLoader(dataset, batch_size=64)\n",
    "for batch in data_loader:\n",
    "    # Aquí el batch tiene automáticamente 64 elementos, ya está organizado\n",
    "    ...\n",
    "```\n",
    "\n",
    "### 3. **Shuffling y Aleatoriedad**\n",
    "El **DataLoader de PyTorch** permite el **shuffling** (mezcla aleatoria) de los datos para garantizar que el modelo no se sobreajuste a un patrón específico del orden de los datos. Esto es muy útil en cada **epoch** de entrenamiento, ya que el modelo debe ver los datos en un orden diferente cada vez.\n",
    "\n",
    "- En un generador o iterador en Python, tendrías que mezclar manualmente los datos antes de cada iteración si quisieras este comportamiento.\n",
    "\n",
    "#### Ejemplo de shuffling:\n",
    "```python\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "```\n",
    "\n",
    "- Esto asegura que el orden de los datos se baraje en cada iteración, lo cual es fundamental para un entrenamiento robusto.\n",
    "\n",
    "### 4. **Soporte para collate_fn**\n",
    "El **DataLoader** de PyTorch permite definir una función personalizada de cómo agrupar las muestras (`collate_fn`), lo cual es muy útil cuando se trabaja con datos de diferentes formas o tamaños, como secuencias de texto, imágenes de diferentes resoluciones, etc. Este tipo de manipulación compleja de datos no es tan fácil de hacer con un simple iterador o generador en Python.\n",
    "\n",
    "#### Ejemplo de `collate_fn`:\n",
    "```python\n",
    "def custom_collate_fn(batch):\n",
    "    # Lógica personalizada para agrupar el batch\n",
    "    ...\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=64, collate_fn=custom_collate_fn)\n",
    "```\n",
    "\n",
    "### 5. **Compatibilidad con GPUs y optimización de la memoria**\n",
    "El **DataLoader de PyTorch** está optimizado para trabajar con GPUs y manejar grandes cantidades de datos eficientemente. Los datos se pueden transferir de forma automática desde la CPU a la GPU si se utilizan las técnicas adecuadas.\n",
    "\n",
    "- Los generadores de Python no tienen este tipo de optimización interna para mover datos entre dispositivos o gestionar memoria de forma eficiente cuando se usan GPUs, lo cual es crítico en PyTorch.\n",
    "\n",
    "### 6. **Integración con otros componentes de PyTorch**\n",
    "El **DataLoader** está completamente integrado con el ecosistema de PyTorch, lo que permite una interacción fluida con el optimizador, las capas del modelo y el sistema de cálculo automático de gradientes (`autograd`). Además, el DataLoader maneja de manera eficiente grandes conjuntos de datos, como los proporcionados por `torchvision.datasets` o `torchtext`, los cuales pueden incluir funciones adicionales como preprocesamiento, augmentación de datos o descarga de los datos.\n",
    "\n",
    "### 7. **Prefetching y Pipeline de Datos**\n",
    "El **DataLoader** permite la optimización de la lectura de datos mediante técnicas como **prefetching**, es decir, mientras se está entrenando con un lote, se comienza a cargar el siguiente en memoria. Este tipo de optimización es muy difícil de implementar en Python puro sin usar bibliotecas adicionales o técnicas avanzadas de multiprocesamiento y concurrencia.\n",
    "\n",
    "### Diferencias Clave entre DataLoader de PyTorch y los Iteradores de Python\n",
    "\n",
    "| Característica                    | Iterador en Python            | DataLoader en PyTorch                 |\n",
    "|-----------------------------------|-------------------------------|---------------------------------------|\n",
    "| **Paralelismo**                   | No soporta multiprocesamiento | Soporta multiprocesamiento con `num_workers` |\n",
    "| **Manejo de batches**             | No es automático               | Automáticamente agrupa los datos en batches |\n",
    "| **Shuffling**                     | No soporta                     | Soporta mezcla aleatoria de datos (`shuffle=True`) |\n",
    "| **Collate Function**              | Debes implementar manualmente  | Soporta `collate_fn` para manipulación avanzada de datos |\n",
    "| **Optimización para GPUs**        | No optimizado                  | Optimizado para trabajar con GPUs y sistemas distribuidos |\n",
    "| **Prefetching y pipelines**       | No soporta                     | Prefetching de datos en memoria para mejorar el rendimiento |\n",
    "| **Integración con PyTorch**       | No hay integración nativa      | Totalmente integrado con PyTorch para trabajar con optimizadores, modelos, etc. |\n",
    "\n",
    "### Conclusión\n",
    "Los **DataLoaders de PyTorch** están diseñados específicamente para las necesidades del entrenamiento de modelos de deep learning, donde se manejan grandes volúmenes de datos y se requiere un procesamiento rápido, eficiente y paralelo. Aunque los **generadores e iteradores de Python** también permiten iterar sobre datos de manera perezosa, no están optimizados para tareas de aprendizaje profundo que involucran el uso de GPUs, la carga en paralelo de datos o la creación automática de batches, por lo que el **DataLoader** de PyTorch es más adecuado en estos casos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factored-minstral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
